# Quiz for FastAI lesson 4

A little more from [fastai lesson 3](https://course.fast.ai/videos/?lesson=3)

1. With the `fastai.text` library, what is numericalization?

1. What is an activation function (non-linearity)?

1. What shape is a sigmoid function? How popular are sigmoid functions as activation functions?

1. What is a Rectified Linear Unit (ReLU)? What does it look like? How is it computed?

1. What is the `Universal approximation theorem`?

[fastai lesson 4](https://course.fast.ai/videos/?lesson=4)

1. True / False : Deep learning has not traditionally been good at NLP classification problems.

1. What is transfer learning? Why is it especially helpful for NLP problems?

1. What is a weakness of n-gram language models that Jeremy discusses?

1. Language models don't only encode grammatical information. What else is encoded in a language model?

1. What in the difference between supervised, *un*supervised, and *self-*supervised machine learning?

1. Jeremy limits his language model vocabulary to 60,000. This leaves many tokens as `xxunk`. What does that symbol mean, and what are the effects of having lots of `xxunk`s in the text?

1. Would a vocabulary limit of 60,000 work well for all natural languages?

1. What does `xxup` mean? Why might it be helpful?

1. What is the encoder? What is its purpose?

1. What is the difference between continuous and categorical variables?

1. What is a sparse matrix? How do we usually store sparse matrices?

1. What is the `cold start problem`?

1. Any questions about the MS Excel stuff?

1. What text editor does Jeremy use?

1. What is an embedding?

1. I ***highly*** recommend that you rewatch the [last section](https://youtu.be/C9UdVPE3ynA?t=5484) on terminology until you are comfortable with all of it.
